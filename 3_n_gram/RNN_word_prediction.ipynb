{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CS584_Homework 3_Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre-Requirement: <br>\n",
    "**tensorflow**<br>\n",
    "**keras**<br>\n",
    "**matplotlib**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "887521\n",
      "['aer', 'banknote', 'berlitz', 'calloway', 'centrust', 'cluett', 'fromstein', 'gitano', 'guterman', 'hydroquebec', 'ipo', 'kia', 'memotec', 'mlx', 'nahb', 'punts', 'rake', 'regatta', 'rubens', 'sim', 'snackfood', 'ssangyong', 'swapo', 'wachter', 'pierre', '<unk>', 'N', 'years', 'old', 'will', 'join', 'the', 'board', 'as', 'a', 'nonexecutive', 'director', 'nov', 'N', 'mr', '<unk>', 'is', 'chairman', 'of', '<unk>', 'nv', 'the', 'dutch', 'publishing', 'group', 'rudolph', '<unk>', 'N', 'years', 'old', 'and', 'former', 'chairman', 'of', 'consolidated', 'gold', 'fields', 'plc', 'was', 'named', 'a', 'nonexecutive', 'director', 'of', 'this', 'british', 'industrial', 'conglomerate', 'a', 'form', 'of', 'asbestos', 'once', 'used', 'to', 'make', 'kent', 'cigarette', 'filters', 'has', 'caused', 'a', 'high', 'percentage', 'of', 'cancer', 'deaths', 'among', 'a', 'group', 'of', 'workers', 'exposed', 'to', 'it']\n",
      "887521\n"
     ]
    }
   ],
   "source": [
    "file_1 = 'a3-data/train.txt'\n",
    "file_2 = 'a3-data/valid.txt'\n",
    "\n",
    "def clean_text(filepath):\n",
    "    \n",
    "    f = open('a3-data/train.txt',\"r\", encoding='utf-8')\n",
    "    lines = f.readlines()\n",
    "    output = []\n",
    "    for i in range(0,lines.__len__(),1):\n",
    "        for word in lines[i].split():\n",
    "            word = word.strip(' ')\n",
    "            word = re.sub(r\"[-()\\\"#/@;:{}'+=|.!?,]\", \"\", word)\n",
    "            output.append(word)\n",
    "    \n",
    "    return output\n",
    "\n",
    "train_word = clean_text(file_1)\n",
    "val_word = clean_text(file_2)\n",
    "\n",
    "print(len(train_word))\n",
    "print(train_word[0:100])\n",
    "print(len(val_word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build and Train the RNN Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "import numpy as np\n",
    "from keras.utils import to_categorical\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, LSTM, Embedding\n",
    "from pickle import dump,load\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras import optimizers\n",
    "from keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert the whole text into text sequences of four words \n",
    "train_len = 3+1\n",
    "text_sequences = []\n",
    "for i in range(train_len,len(train_word)):\n",
    "    seq = train_word[i-train_len:i]\n",
    "    text_sequences.append(seq)\n",
    "\n",
    "sequences = {}\n",
    "count = 1\n",
    "for i in range(len(train_word)):\n",
    "    if train_word[i] not in sequences:\n",
    "        sequences[train_word[i]] = count\n",
    "        count += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we convert the data into a numpy array\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(text_sequences)\n",
    "sequences = tokenizer.texts_to_sequences(text_sequences) \n",
    "\n",
    "unique_words = tokenizer.index_word\n",
    "unique_wordsApp = tokenizer.word_counts\n",
    "vocabulary_size = len(tokenizer.word_counts)\n",
    "\n",
    "n_sequences = np.empty([len(sequences),train_len], dtype='int32')\n",
    "for i in range(len(sequences)):\n",
    "    n_sequences[i] = sequences[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(887517, 3)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splits the sequences into inputs and output labels\n",
    "# One-hot embedding\n",
    "train_inputs = n_sequences[:,:-1]\n",
    "train_targets = n_sequences[:,-1]\n",
    "\n",
    "train_targets = to_categorical(train_targets, num_classes=vocabulary_size+1)\n",
    "seq_len = train_inputs.shape[1]\n",
    "train_inputs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Model\n",
    "def create_model(vocabulary_size, seq_len):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(vocabulary_size, seq_len, embeddings_initializer='uniform', input_length=seq_len))\n",
    "    model.add(LSTM(50,return_sequences=True))\n",
    "    model.add(LSTM(50))\n",
    "    model.add(Dense(50,activation='relu'))\n",
    "    model.add(Dense(vocabulary_size,activation='softmax'))\n",
    "    opt_adam = optimizers.adam(lr=1e-3)\n",
    "    model.compile(loss='categorical_crossentropy',optimizer=opt_adam,metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_1 (Embedding)      (None, 3, 3)              29826     \n",
      "_________________________________________________________________\n",
      "lstm_1 (LSTM)                (None, 3, 50)             10800     \n",
      "_________________________________________________________________\n",
      "lstm_2 (LSTM)                (None, 50)                20200     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                2550      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 9942)              507042    \n",
      "=================================================================\n",
      "Total params: 570,418\n",
      "Trainable params: 570,418\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = create_model(vocabulary_size+1,seq_len)\n",
    "path = './checkpoints/word_pred_Model4.h5'\n",
    "checkpoint = ModelCheckpoint(path, monitor='loss', verbose=1, save_best_only=True, mode='min')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyp/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "887517/887517 [==============================] - 173s 195us/step - loss: 6.3414 - accuracy: 0.1075\n",
      "\n",
      "Epoch 00001: loss improved from inf to 6.34141, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 2/30\n",
      "887517/887517 [==============================] - 180s 202us/step - loss: 5.8188 - accuracy: 0.1484\n",
      "\n",
      "Epoch 00002: loss improved from 6.34141 to 5.81877, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 3/30\n",
      "887517/887517 [==============================] - 199s 224us/step - loss: 5.5998 - accuracy: 0.1620\n",
      "\n",
      "Epoch 00003: loss improved from 5.81877 to 5.59980, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 4/30\n",
      "887517/887517 [==============================] - 190s 215us/step - loss: 5.4905 - accuracy: 0.1690\n",
      "\n",
      "Epoch 00004: loss improved from 5.59980 to 5.49050, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 5/30\n",
      "887517/887517 [==============================] - 198s 223us/step - loss: 5.4234 - accuracy: 0.1734\n",
      "\n",
      "Epoch 00005: loss improved from 5.49050 to 5.42341, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 6/30\n",
      "887517/887517 [==============================] - 213s 240us/step - loss: 5.3764 - accuracy: 0.1761\n",
      "\n",
      "Epoch 00006: loss improved from 5.42341 to 5.37636, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 7/30\n",
      "887517/887517 [==============================] - 246s 277us/step - loss: 5.3404 - accuracy: 0.1780\n",
      "\n",
      "Epoch 00007: loss improved from 5.37636 to 5.34040, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 8/30\n",
      "887517/887517 [==============================] - 220s 248us/step - loss: 5.3122 - accuracy: 0.1796\n",
      "\n",
      "Epoch 00008: loss improved from 5.34040 to 5.31224, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 9/30\n",
      "887517/887517 [==============================] - 240s 270us/step - loss: 5.2887 - accuracy: 0.1810\n",
      "\n",
      "Epoch 00009: loss improved from 5.31224 to 5.28867, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 10/30\n",
      "887517/887517 [==============================] - 224s 252us/step - loss: 5.2691 - accuracy: 0.1822\n",
      "\n",
      "Epoch 00010: loss improved from 5.28867 to 5.26911, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 11/30\n",
      "887517/887517 [==============================] - 224s 253us/step - loss: 5.2522 - accuracy: 0.1831\n",
      "\n",
      "Epoch 00011: loss improved from 5.26911 to 5.25218, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 12/30\n",
      "887517/887517 [==============================] - 233s 263us/step - loss: 5.2371 - accuracy: 0.1840\n",
      "\n",
      "Epoch 00012: loss improved from 5.25218 to 5.23710, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 13/30\n",
      "887517/887517 [==============================] - 233s 263us/step - loss: 5.2243 - accuracy: 0.1852\n",
      "\n",
      "Epoch 00013: loss improved from 5.23710 to 5.22427, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 14/30\n",
      "887517/887517 [==============================] - 235s 265us/step - loss: 5.2123 - accuracy: 0.1858\n",
      "\n",
      "Epoch 00014: loss improved from 5.22427 to 5.21234, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 15/30\n",
      "887517/887517 [==============================] - 232s 262us/step - loss: 5.2024 - accuracy: 0.1867\n",
      "\n",
      "Epoch 00015: loss improved from 5.21234 to 5.20235, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 16/30\n",
      "887517/887517 [==============================] - 230s 259us/step - loss: 5.1927 - accuracy: 0.1874\n",
      "\n",
      "Epoch 00016: loss improved from 5.20235 to 5.19265, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 17/30\n",
      "887517/887517 [==============================] - 218s 245us/step - loss: 5.1842 - accuracy: 0.1881\n",
      "\n",
      "Epoch 00017: loss improved from 5.19265 to 5.18416, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 18/30\n",
      "887517/887517 [==============================] - 224s 252us/step - loss: 5.1762 - accuracy: 0.1885\n",
      "\n",
      "Epoch 00018: loss improved from 5.18416 to 5.17621, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 19/30\n",
      "887517/887517 [==============================] - 234s 264us/step - loss: 5.1689 - accuracy: 0.1890\n",
      "\n",
      "Epoch 00019: loss improved from 5.17621 to 5.16893, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 20/30\n",
      "887517/887517 [==============================] - 228s 257us/step - loss: 5.1622 - accuracy: 0.1897\n",
      "\n",
      "Epoch 00020: loss improved from 5.16893 to 5.16222, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 21/30\n",
      "887517/887517 [==============================] - 231s 260us/step - loss: 5.1563 - accuracy: 0.1902\n",
      "\n",
      "Epoch 00021: loss improved from 5.16222 to 5.15628, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 22/30\n",
      "887517/887517 [==============================] - 232s 262us/step - loss: 5.1507 - accuracy: 0.1905\n",
      "\n",
      "Epoch 00022: loss improved from 5.15628 to 5.15073, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 23/30\n",
      "887517/887517 [==============================] - 240s 270us/step - loss: 5.1456 - accuracy: 0.1908\n",
      "\n",
      "Epoch 00023: loss improved from 5.15073 to 5.14555, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 24/30\n",
      "887517/887517 [==============================] - 235s 264us/step - loss: 5.1403 - accuracy: 0.1914\n",
      "\n",
      "Epoch 00024: loss improved from 5.14555 to 5.14035, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 25/30\n",
      "887517/887517 [==============================] - 219s 247us/step - loss: 5.1354 - accuracy: 0.1918\n",
      "\n",
      "Epoch 00025: loss improved from 5.14035 to 5.13541, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 26/30\n",
      "887517/887517 [==============================] - 234s 264us/step - loss: 5.1311 - accuracy: 0.1921\n",
      "\n",
      "Epoch 00026: loss improved from 5.13541 to 5.13110, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 27/30\n",
      "887517/887517 [==============================] - 227s 256us/step - loss: 5.1269 - accuracy: 0.1922\n",
      "\n",
      "Epoch 00027: loss improved from 5.13110 to 5.12692, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 28/30\n",
      "887517/887517 [==============================] - 227s 256us/step - loss: 5.1230 - accuracy: 0.1925\n",
      "\n",
      "Epoch 00028: loss improved from 5.12692 to 5.12296, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 29/30\n",
      "887517/887517 [==============================] - 246s 277us/step - loss: 5.1195 - accuracy: 0.1927\n",
      "\n",
      "Epoch 00029: loss improved from 5.12296 to 5.11948, saving model to ./checkpoints/word_pred_Model4.h5\n",
      "Epoch 30/30\n",
      "887517/887517 [==============================] - 237s 267us/step - loss: 5.1154 - accuracy: 0.1932\n",
      "\n",
      "Epoch 00030: loss improved from 5.11948 to 5.11539, saving model to ./checkpoints/word_pred_Model4.h5\n"
     ]
    }
   ],
   "source": [
    "# Training Model\n",
    "history = model.fit(train_inputs,train_targets,batch_size=64,epochs=30,verbose=1,callbacks=[checkpoint])\n",
    "model.save('word_pred_Model4.h5')\n",
    "dump(tokenizer,open('tokenizer_Model4','wb'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[567.59811404 336.55675677 270.37322764 242.37735823 226.65103514\n",
      " 216.2341526  208.59687469 202.80349522 198.07991155 194.24295803\n",
      " 190.98123503 188.12463478 185.72524491 183.52260941 181.69899056\n",
      " 179.94543209 178.4241834  177.00980428 175.72619507 174.55102628\n",
      " 173.51845796 172.55804986 171.66648331 170.77544128 169.93451296\n",
      " 169.20343687 168.49655727 167.83115889 167.24920517 166.56503515]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'Training Loss')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot training loss and acc.\n",
    "# Calculate the preplexity value\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "loss = history.history['loss']\n",
    "loss = np.asarray(loss)\n",
    "preplexity = np.exp(loss)\n",
    "print(preplexity)\n",
    "\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.title('Training Loss')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Above results shows the preplexity value for each epoch.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/cyp/anaconda3/lib/python3.7/site-packages/tensorflow_core/python/framework/indexed_slices.py:433: UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n",
      "  \"Converting sparse IndexedSlices to a dense Tensor of unknown shape. \"\n"
     ]
    }
   ],
   "source": [
    "pred_model = load_model('word_pred_Model4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = load(open('tokenizer_Model4','rb'))\n",
    "seq_len = 3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_text(model, tokenizer, seq_len, seed_text, num_gen_words):\n",
    "    output_text = []\n",
    "    input_text = seed_text\n",
    "    for i in range(num_gen_words):\n",
    "        encoded_text = tokenizer.texts_to_sequences([input_text])[0]\n",
    "        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len,truncating='pre')\n",
    "        pred_word_ind = model.predict_classes(pad_encoded,verbose=0)[0]\n",
    "        \n",
    "        pred_word = tokenizer.index_word[pred_word_ind]\n",
    "        input_text += ' '+pred_word\n",
    "        output_text.append(pred_word)\n",
    "    return ' '.join(output_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 The next word of fall is : to\n",
      "2 The next word of failed is : to\n",
      "3 The next word of floor is : the\n",
      "4 The next word of plate is : in\n",
      "5 The next word of futures is : and\n",
      "6 The next word of capital is : and\n",
      "7 The next word of the is : <unk>\n",
      "8 The next word of it is : s\n",
      "9 The next word of banks is : and\n",
      "10 The next word of is is : a\n",
      "11 The next word of their is : <unk>\n",
      "12 The next word of in is : the\n",
      "13 The next word of for is : the\n",
      "14 The next word of were is : <unk>\n",
      "15 The next word of only is : the\n",
      "16 The next word of the is : <unk>\n",
      "17 The next word of extremely is : <unk>\n",
      "18 The next word of placed is : by\n",
      "19 The next word of big is : <unk>\n",
      "20 The next word of trading is : in\n",
      "21 The next word of the is : <unk>\n",
      "22 The next word of after is : the\n",
      "23 The next word of news is : of\n",
      "24 The next word of rid is : of\n",
      "25 The next word of declared is : the\n",
      "26 The next word of stocks is : and\n",
      "27 The next word of was is : <unk>\n",
      "28 The next word of which is : <unk>\n",
      "29 The next word of street is : <unk>\n",
      "30 The next word of the is : <unk>\n"
     ]
    }
   ],
   "source": [
    "test_file = open('a3-data/input.txt',\"r\", encoding='utf-8')\n",
    "lines = test_file.readlines()\n",
    "test_line = lines[:30]\n",
    "i = 1\n",
    "for line in test_line:\n",
    "    line = re.sub(r\"[-()\\\"#/@;:{}'+=_|.!?,]\", \"\", line)\n",
    "    word = line.split()\n",
    "    test_word = word[-1]\n",
    "    out = gen_text(pred_model, tokenizer, seq_len=seq_len, seed_text=test_word, num_gen_words=1)\n",
    "    print(i, \"The next word of\",test_word, \"is :\", out)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We trained model many times. We found that if we reduce the training data to 10000-50000, the prediction results will be improved. The number of training data can be changed by modifying belows two line:<br>\n",
    "**train_inputs = n_sequences[:,:-1][:50000]**<br>\n",
    "**train_targets = n_sequences[:,-1][:50000]**<br>\n",
    "We will continue to improve the model's performance:<br>\n",
    "1. Adopt different methods to clean original text data.\n",
    "2. Modify RNN structure.\n",
    "3. Adjust Hyper-Parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prove that perplexity is exp(total loss /number of predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the original mathematical defintion of **Perplexity**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ 2^{H(p)} = 2^{-\\sum_{x} p(x)log_{2}p(x)} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$ H(p) =  -\\sum_{x} p(x)log_{2}p(x) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above equation is entropy. More importantly, we use entropy to evaluate the similarity and calculate loss value. Therefore, we can re-write Preplexity function as:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$perplexity = 2^{entropy}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$perplexity = 2^{-\\sum_{i=1}^{N}p(x_{i})log_2^{q(xi)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$perplexity = e^{-\\sum_{i=1}^{N}p(x_{i})ln^{q(xi)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$preplexity = e^{-\\sum_{i=1}^{N}\\frac{1}{N}ln^{q(xi)}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$perplexity = \\prod_{i=1}^{N}q(x_{i})^{-1/N} = = \\sqrt[N]{\\frac{1}{q(x_{1})q(x_{2})...q(x_{n})}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, perplexity is exp(total loss / number of predictions)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
