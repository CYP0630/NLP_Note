{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS584 Homework 5: Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seq2Seq: LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* English to French"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparationÂ¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load_doc: loading file \n",
    "to_line: cutting the the whole file to line by line\n",
    "clean_data: remove the symble, url, or other irrevalent information \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def to_line(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return lines\n",
    "\n",
    "def clean_data(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for line in lines:\n",
    "        # normalize unicode characters\n",
    "        line = normalize('NFD', line).encode('utf-8', 'ignore')\n",
    "        #print(line)\n",
    "        line = line.decode('UTF-8')\n",
    "            \n",
    "        # tokenize on white space\n",
    "        line = line.split()\n",
    "            \n",
    "        # convert to lowercase\n",
    "        line = [word.lower() for word in line]\n",
    "            \n",
    "        # remove punctuation from each token\n",
    "        line = [word.translate(table) for word in line]\n",
    "            \n",
    "         # remove non-printable chars form each token\n",
    "        line = [re_print.sub('', w) for w in line]\n",
    "        #print(line)\n",
    "        # remove tokens with numbers in them\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "            \n",
    "        # store as string\n",
    "        cleaned.append(' '.join(line))\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentencens match with French sentences \n",
      "\n",
      "English sentence: 2007723 \n",
      "\n",
      "French sentence: 2007723 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the first 30000 sentences to contruct dataset\n",
    "data_len = 30000\n",
    "\n",
    "en = load_doc('europarl-v7.fr-en.en')\n",
    "en_lines = to_line(en)\n",
    "fr = load_doc('europarl-v7.fr-en.fr')\n",
    "fr_lines = to_line(fr)\n",
    "\n",
    "if len(en_lines) == len(fr_lines):\n",
    "    print('English sentencens match with French sentences \\n')\n",
    "    print('English sentence:', len(en_lines), '\\n')\n",
    "    print('French sentence:', len(fr_lines), '\\n')\n",
    "else :\n",
    "    print('English sentencens does not match with French sentences')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning english part and french part \n",
    "clean_en = clean_data(en_lines)[0:data_len]\n",
    "clean_fr = clean_data(fr_lines)[0:data_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i should also like to make a few comments firstly mr berend regarding the assessment you have made of this sixth periodic report\n",
      "je voudrais a mon tour faire quelques observations dabord sur le jugement que vous portez monsieur le rapporteur sur ce sixieme rapport periodique\n"
     ]
    }
   ],
   "source": [
    "clean_en = np.array(clean_en)\n",
    "clean_fr = np.array(clean_fr)\n",
    "\n",
    "# A showcase \n",
    "print(clean_en[700])\n",
    "print(clean_fr[700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_texts:  (30000,)\n",
      "Length of target_texts: (30000,)\n",
      "max length of input  sentences: 32\n",
      "max length of target sentences: 34\n"
     ]
    }
   ],
   "source": [
    "input_texts = clean_en\n",
    "target_texts = ['\\t' + text + '\\n' for text in clean_fr]\n",
    "\n",
    "print('Length of input_texts:  ' + str(input_texts.shape))\n",
    "print('Length of target_texts: ' + str(input_texts.shape))\n",
    "\n",
    "#This dataset includes lots of long sentence\n",
    "#But, if we set max length with a large number, CPU cannot handle that, so we set max_encode is 32\n",
    "#Becuase we add '\\t' and '\\n' to target, so the max_decode is 34\n",
    "max_encoder_seq_length = 32 \n",
    "max_decoder_seq_length = 34\n",
    "\n",
    "print('max length of input  sentences: %d' % (max_encoder_seq_length))\n",
    "print('max length of target sentences: %d' % (max_decoder_seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Text to Sequence\n",
    "* One-hot embedding the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoder_input_seq: (30000, 32)\n",
      "shape of input_token_index: 27\n",
      "shape of decoder_input_seq: (30000, 34)\n",
      "shape of target_token_index: 29\n"
     ]
    }
   ],
   "source": [
    "def text2sequences(max_len, lines):\n",
    "    tokenizer = Tokenizer(char_level=True, filters='')\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    seqs = tokenizer.texts_to_sequences(lines)\n",
    "    seqs_pad = pad_sequences(seqs, maxlen=max_len, padding='post')\n",
    "    return seqs_pad, tokenizer.word_index\n",
    "\n",
    "encoder_input_seq, input_token_index = text2sequences(max_encoder_seq_length, \n",
    "                                                      input_texts)\n",
    "decoder_input_seq, target_token_index = text2sequences(max_decoder_seq_length, \n",
    "                                                       target_texts)\n",
    "\n",
    "print('shape of encoder_input_seq: ' + str(encoder_input_seq.shape))\n",
    "print('shape of input_token_index: ' + str(len(input_token_index)))\n",
    "print('shape of decoder_input_seq: ' + str(decoder_input_seq.shape))\n",
    "print('shape of target_token_index: ' + str(len(target_token_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_encoder_tokens: 28\n",
      "num_decoder_tokens: 30\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens = len(input_token_index) +1\n",
    "num_decoder_tokens = len(target_token_index) +1\n",
    "\n",
    "print('num_encoder_tokens: ' + str(num_encoder_tokens))\n",
    "print('num_decoder_tokens: ' + str(num_decoder_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(sequences, max_len, vocab_size):\n",
    "    n = len(sequences)\n",
    "    data = np.zeros((n, max_len, vocab_size))\n",
    "    for i in range(n):\n",
    "        data[i, :, :] = to_categorical(sequences[i], num_classes=vocab_size)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 32, 28)\n",
      "(30000, 34, 30)\n",
      "(30000, 34, 30)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data = onehot_encode(encoder_input_seq, max_encoder_seq_length, num_encoder_tokens)\n",
    "decoder_input_data = onehot_encode(decoder_input_seq, max_decoder_seq_length, num_decoder_tokens)\n",
    "\n",
    "decoder_target_seq = np.zeros(decoder_input_seq.shape)\n",
    "decoder_target_seq[:, 0:-1] = decoder_input_seq[:, 1:]\n",
    "decoder_target_data = onehot_encode(decoder_target_seq, \n",
    "                                    max_decoder_seq_length, \n",
    "                                    num_decoder_tokens)\n",
    "\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Split the dataset to training part and validation part**\n",
    "* **24000 training sentence, 6000 validation sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_indices = np.random.permutation(data_len)\n",
    "\n",
    "train_count = int(np.floor(data_len * 0.8))\n",
    "train_indices = rand_indices[0:train_count]\n",
    "val_indices = rand_indices[train_count:data_len]\n",
    "\n",
    "input_texts_train = input_texts[train_indices]\n",
    "target_texts_train = np.asarray(target_texts)[train_indices]\n",
    "\n",
    "encoder_input_train = encoder_input_data[train_indices]\n",
    "decoder_input_train = decoder_input_data[train_indices]\n",
    "decoder_target_train = decoder_target_data[train_indices]\n",
    "\n",
    "input_texts_val = input_texts[val_indices]\n",
    "target_texts_val = np.asarray(target_texts)[val_indices]\n",
    "encoder_input_val = encoder_input_data[val_indices]\n",
    "decoder_input_val = decoder_input_data[val_indices]\n",
    "decoder_target_val = decoder_target_data[val_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "encoder_inputs (InputLayer)  (None, None, 28)          0         \n",
      "_________________________________________________________________\n",
      "encoder_lstm (LSTM)          [(None, 256), (None, 256) 291840    \n",
      "=================================================================\n",
      "Total params: 291,840\n",
      "Trainable params: 291,840\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, LSTM\n",
    "from keras.models import Model\n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "# inputs of the encoder network\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), \n",
    "                       name='encoder_inputs')\n",
    "\n",
    "# set the LSTM layer\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True, \n",
    "                    dropout=0.5, name='encoder_lstm')\n",
    "_, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "# build the encoder network model\n",
    "encoder_model = Model(inputs=encoder_inputs, \n",
    "                      outputs=[state_h, state_c],\n",
    "                      name='encoder')\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input_x (InputLayer)    (None, None, 30)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_h (InputLayer)    (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_c (InputLayer)    (None, 256)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 256),  293888      decoder_input_x[0][0]            \n",
      "                                                                 decoder_input_h[0][0]            \n",
      "                                                                 decoder_input_c[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 30)     7710        decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 301,598\n",
      "Trainable params: 301,598\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input, LSTM, Dense\n",
    "from keras.models import Model\n",
    "\n",
    "# inputs of the decoder network\n",
    "decoder_input_h = Input(shape=(latent_dim,), name='decoder_input_h')\n",
    "decoder_input_c = Input(shape=(latent_dim,), name='decoder_input_c')\n",
    "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
    "\n",
    "# set the LSTM layer\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, \n",
    "                    return_state=True, dropout=0.5, name='decoder_lstm')\n",
    "decoder_lstm_outputs, state_h, state_c = decoder_lstm(decoder_input_x, \n",
    "                                                      initial_state=[decoder_input_h, decoder_input_c])\n",
    "\n",
    "# set the dense layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "decoder_outputs = decoder_dense(decoder_lstm_outputs)\n",
    "\n",
    "# build the decoder network model\n",
    "decoder_model = Model(inputs=[decoder_input_x, decoder_input_h, decoder_input_c],\n",
    "                      outputs=[decoder_outputs, state_h, state_c],\n",
    "                      name='decoder')\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder-Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_training\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input_x (InputLayer)    (None, None, 28)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_x (InputLayer)    (None, None, 30)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 256), (None, 291840      encoder_input_x[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 256),  293888      decoder_input_x[0][0]            \n",
      "                                                                 encoder[1][0]                    \n",
      "                                                                 encoder[1][1]                    \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 30)     7710        decoder_lstm[1][0]               \n",
      "==================================================================================================\n",
      "Total params: 593,438\n",
      "Trainable params: 593,438\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# input layers\n",
    "encoder_input_x = Input(shape=(None, num_encoder_tokens), name='encoder_input_x')\n",
    "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
    "\n",
    "# connect encoder to decoder\n",
    "encoder_final_states = encoder_model([encoder_input_x])\n",
    "decoder_lstm_output, _, _ = decoder_lstm(decoder_input_x, initial_state=encoder_final_states)\n",
    "decoder_pred = decoder_dense(decoder_lstm_output)\n",
    "\n",
    "model = Model(inputs=[encoder_input_x, decoder_input_x], \n",
    "              outputs=decoder_pred, \n",
    "              name='model_training')\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "24000/24000 [==============================] - 23s 955us/step - loss: 2.4671 - val_loss: 2.0818\n",
      "Epoch 2/50\n",
      "24000/24000 [==============================] - 22s 915us/step - loss: 2.2440 - val_loss: 1.9363\n",
      "Epoch 3/50\n",
      "24000/24000 [==============================] - 22s 912us/step - loss: 2.1595 - val_loss: 1.8340\n",
      "Epoch 4/50\n",
      "24000/24000 [==============================] - 22s 909us/step - loss: 2.1042 - val_loss: 1.7860\n",
      "Epoch 5/50\n",
      "24000/24000 [==============================] - 22s 908us/step - loss: 2.0597 - val_loss: 1.6961\n",
      "Epoch 6/50\n",
      "24000/24000 [==============================] - 22s 911us/step - loss: 2.0255 - val_loss: 1.6529\n",
      "Epoch 7/50\n",
      "24000/24000 [==============================] - 22s 899us/step - loss: 1.9953 - val_loss: 1.6131\n",
      "Epoch 8/50\n",
      "24000/24000 [==============================] - 22s 924us/step - loss: 1.9724 - val_loss: 1.5861\n",
      "Epoch 9/50\n",
      "24000/24000 [==============================] - 22s 915us/step - loss: 1.9520 - val_loss: 1.5922\n",
      "Epoch 10/50\n",
      "24000/24000 [==============================] - 22s 919us/step - loss: 1.9372 - val_loss: 1.5329\n",
      "Epoch 11/50\n",
      "24000/24000 [==============================] - 22s 916us/step - loss: 1.9223 - val_loss: 1.5094\n",
      "Epoch 12/50\n",
      "24000/24000 [==============================] - 22s 919us/step - loss: 1.9107 - val_loss: 1.4978\n",
      "Epoch 13/50\n",
      "24000/24000 [==============================] - 22s 915us/step - loss: 1.8976 - val_loss: 1.6041\n",
      "Epoch 14/50\n",
      "24000/24000 [==============================] - 22s 911us/step - loss: 1.8862 - val_loss: 1.4756\n",
      "Epoch 15/50\n",
      "24000/24000 [==============================] - 22s 922us/step - loss: 1.8764 - val_loss: 1.4560\n",
      "Epoch 16/50\n",
      "24000/24000 [==============================] - 22s 919us/step - loss: 1.8686 - val_loss: 1.4412\n",
      "Epoch 17/50\n",
      "24000/24000 [==============================] - 22s 925us/step - loss: 1.8590 - val_loss: 1.4737\n",
      "Epoch 18/50\n",
      "24000/24000 [==============================] - 22s 911us/step - loss: 1.8495 - val_loss: 1.4485\n",
      "Epoch 19/50\n",
      "24000/24000 [==============================] - 22s 908us/step - loss: 1.8449 - val_loss: 1.4195\n",
      "Epoch 20/50\n",
      "24000/24000 [==============================] - 22s 924us/step - loss: 1.8381 - val_loss: 1.4051\n",
      "Epoch 21/50\n",
      "24000/24000 [==============================] - 22s 903us/step - loss: 1.8315 - val_loss: 1.4013\n",
      "Epoch 22/50\n",
      "24000/24000 [==============================] - 22s 917us/step - loss: 1.8254 - val_loss: 1.3868\n",
      "Epoch 23/50\n",
      "24000/24000 [==============================] - 22s 920us/step - loss: 1.8239 - val_loss: 1.3859\n",
      "Epoch 24/50\n",
      "24000/24000 [==============================] - 22s 929us/step - loss: 1.8174 - val_loss: 1.3748\n",
      "Epoch 25/50\n",
      "24000/24000 [==============================] - 22s 921us/step - loss: 1.8136 - val_loss: 1.3747\n",
      "Epoch 26/50\n",
      "24000/24000 [==============================] - 22s 911us/step - loss: 1.8028 - val_loss: 1.3613\n",
      "Epoch 27/50\n",
      "24000/24000 [==============================] - 22s 920us/step - loss: 1.8043 - val_loss: 1.3569\n",
      "Epoch 28/50\n",
      "24000/24000 [==============================] - 22s 911us/step - loss: 1.7962 - val_loss: 1.3604\n",
      "Epoch 29/50\n",
      "24000/24000 [==============================] - 22s 914us/step - loss: 1.7916 - val_loss: 1.3574\n",
      "Epoch 30/50\n",
      "24000/24000 [==============================] - 22s 920us/step - loss: 1.7888 - val_loss: 1.3457\n",
      "Epoch 31/50\n",
      "24000/24000 [==============================] - 22s 916us/step - loss: 1.7848 - val_loss: 1.3406\n",
      "Epoch 32/50\n",
      "24000/24000 [==============================] - 22s 910us/step - loss: 1.7817 - val_loss: 1.3353\n",
      "Epoch 33/50\n",
      "24000/24000 [==============================] - 22s 911us/step - loss: 1.7808 - val_loss: 1.3391\n",
      "Epoch 34/50\n",
      "24000/24000 [==============================] - 22s 910us/step - loss: 1.7762 - val_loss: 1.3285\n",
      "Epoch 35/50\n",
      "24000/24000 [==============================] - 22s 926us/step - loss: 1.7723 - val_loss: 1.4231\n",
      "Epoch 36/50\n",
      "24000/24000 [==============================] - 22s 928us/step - loss: 1.7678 - val_loss: 1.3246\n",
      "Epoch 37/50\n",
      "24000/24000 [==============================] - 22s 922us/step - loss: 1.7657 - val_loss: 1.3264\n",
      "Epoch 38/50\n",
      "24000/24000 [==============================] - 22s 921us/step - loss: 1.7605 - val_loss: 1.3153\n",
      "Epoch 39/50\n",
      "24000/24000 [==============================] - 22s 916us/step - loss: 1.7632 - val_loss: 1.3102\n",
      "Epoch 40/50\n",
      "24000/24000 [==============================] - 22s 910us/step - loss: 1.7580 - val_loss: 1.3126\n",
      "Epoch 41/50\n",
      "24000/24000 [==============================] - 22s 904us/step - loss: 1.7541 - val_loss: 1.3230\n",
      "Epoch 42/50\n",
      "24000/24000 [==============================] - 22s 927us/step - loss: 1.7534 - val_loss: 1.3055\n",
      "Epoch 43/50\n",
      "24000/24000 [==============================] - 22s 925us/step - loss: 1.7501 - val_loss: 1.3023\n",
      "Epoch 44/50\n",
      "24000/24000 [==============================] - 22s 923us/step - loss: 1.7489 - val_loss: 1.3123\n",
      "Epoch 45/50\n",
      "24000/24000 [==============================] - 22s 927us/step - loss: 1.7456 - val_loss: 1.3047\n",
      "Epoch 46/50\n",
      "24000/24000 [==============================] - 22s 907us/step - loss: 1.7464 - val_loss: 1.2990\n",
      "Epoch 47/50\n",
      "24000/24000 [==============================] - 22s 929us/step - loss: 1.7400 - val_loss: 1.2973\n",
      "Epoch 48/50\n",
      "24000/24000 [==============================] - 22s 918us/step - loss: 1.7433 - val_loss: 1.2917\n",
      "Epoch 49/50\n",
      "24000/24000 [==============================] - 22s 910us/step - loss: 1.7440 - val_loss: 1.2902\n",
      "Epoch 50/50\n",
      "24000/24000 [==============================] - 22s 931us/step - loss: 1.7393 - val_loss: 1.2828\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "\n",
    "history = model.fit([encoder_input_train, decoder_input_train],  # training data\n",
    "                      decoder_target_train,                       # labels (left shift of the target sequences)\n",
    "                      batch_size=64, epochs=50, \n",
    "                      validation_data= ([encoder_input_val, decoder_input_val],decoder_target_val))\n",
    "\n",
    "model.save('seq2seq_without_attention.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Calculate BLEU Score on Val Part\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "    \n",
    "    print(len(states_value.shape[0]))\n",
    "    print(target_seq.shape)\n",
    "    \n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # this line of code is greedy selection\n",
    "        # try to use multinomial sampling instead (with temperature)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        if sampled_token_index == 0:\n",
    "            output_tokens[0, -1, sampled_token_index] = 0\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m--------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-29-e9fef02a8f84>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdecode_sequence\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoder_input_val\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-28-bc0eb63a1a60>\u001b[0m in \u001b[0;36mdecode_sequence\u001b[0;34m(input_seq)\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mtarget_seq\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_token_index\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'\\t'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstates_value\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_seq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "decode_sequence(encoder_input_val[1: 2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sentences = []\n",
    "for i in range(encoder_input_val.shape[0]):\n",
    "    decoded_sentences.append(decode_sequence(encoder_input_val[i: i + 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011371222841368092\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import nltk\n",
    "\n",
    "val_scores = []\n",
    "for i in range(target_texts_val.shape[0]):\n",
    "    references = []\n",
    "    split_result = target_texts_val[i][1:-1].split()\n",
    "    references.append(split_result)\n",
    "    candidate = decoded_sentences[i].split()\n",
    "    val_scores.append(sentence_bleu(references, candidate, smoothing_function=SmoothingFunction().method2))\n",
    "\n",
    "val_bleu_score = np.mean(val_scores)\n",
    "print(val_bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **We take 1000 sentences from 40000 to 41000 in original file as the test part**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en = clean_data(en_lines)[data_len+10000:data_len+11000]\n",
    "test_fr = clean_data(fr_lines)[data_len+10000:data_len+11000]\n",
    "\n",
    "test_en = np.array(test_en)\n",
    "test_fr = np.array(test_fr)\n",
    "target_texts_test = ['\\t' + text + '\\n' for text in test_fr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation_sentence(input_sentence):\n",
    "    input_seq_test, test_input_token = text2sequences(len(input_sentence), input_sentence)\n",
    "    max_encoder_seq_length = max(len(line) for line in input_seq_test)\n",
    "    input_x = onehot_encode(input_seq_test, max_encoder_seq_length, num_encoder_tokens)\n",
    "    translated_sentence = decode_sequence(input_x)\n",
    "\n",
    "    return translated_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sentences = []\n",
    "scores_test = []\n",
    "for i in range(len(target_texts_test)):\n",
    "    candidate = translation_sentence(test_en[i])\n",
    "    decoded_sentences.append(candidate)\n",
    "    reference = target_texts_test[i]\n",
    "    score = sentence_bleu(reference, candidate, \n",
    "                          smoothing_function=SmoothingFunction().method2)\n",
    "    scores_test.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.10233211411703365\n"
     ]
    }
   ],
   "source": [
    "test_bleu_score = np.mean(scores_test)\n",
    "print(test_bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save 20 Sentences ExampleÂ¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "English:        i think that the donor conference makes this possible or seems to at least\n",
      "Ground-truth Frehch:  je pense que la conference des donateurs offre ou semble offrir cette possibilite\n",
      "Translation from seq2seq model:  je peut est clos\n",
      "\n",
      "-\n",
      "English:        the third point i would like to make here is that there are parts of greece yes unfortunately greece for example the prefecture of pieria a tourist province in central macedonia which has suffered enormous economic damage because tourists no longer come to that area\n",
      "Ground-truth Frehch:  troisieme point il existe des parties de grece oui de grece malheureusement comme le nome de pierie un nome touristique de la macedoine centrale qui ont subi un enorme prejudice economique du fait que les visiteurs ont deserte la region\n",
      "Translation from seq2seq model:  je debat est clos\n",
      "\n",
      "-\n",
      "English:        such issues should be considered by the european union as well\n",
      "Ground-truth Frehch:  c est un probleme qu il convient aussi d examiner dans le cadre de l union europeenne\n",
      "Translation from seq2seq model:  je peut est clos\n",
      "\n",
      "-\n",
      "English:        mr president i am not about to start arguing the toss with mr swoboda over where the balkans begin\n",
      "Ground-truth Frehch:  monsieur le president je ne souhaite pas me disputer avec mon collegue swoboda sur la question de savoir ou commencent les balkans\n",
      "Translation from seq2seq model:  je debat est clos\n",
      "\n",
      "-\n",
      "English:        in fact it was once said that the balkans begin at the rennweg in vienna\n",
      "Ground-truth Frehch:  on a meme dit quils commencaient aux abords de vienne\n",
      "Translation from seq2seq model:  je peut est clos\n",
      "\n",
      "-\n",
      "English:        but leaving that aside the fact remains that this stability pact is of course adversely affected by the heterogeneity of its states\n",
      "Ground-truth Frehch:  mais si lon excepte cette question le fait est que ce pacte de stabilite souffre naturellement de lheterogeneite des etats concernes\n",
      "Translation from seq2seq model:  je debat est clos\n",
      "\n",
      "-\n",
      "English:        we have first and second wave candidate countries\n",
      "Ground-truth Frehch:  nous avons des pays candidats a ladhesion faisant partie du premier et du deuxieme groupe\n",
      "Translation from seq2seq model:  je peut est clos\n",
      "\n",
      "-\n",
      "English:        we have countries such as croatia and macedonia that are about to overtake candidate countries\n",
      "Ground-truth Frehch:  nous avons des pays comme la croatie et la macedoine qui sont sur le point de depasser les candidats a ladhesion\n",
      "Translation from seq2seq model:  je peut est clos\n",
      "\n",
      "-\n",
      "English:        we have states such as bosniaherzegovina and albania in which the state is scarcely functioning and then the two decisive challenges namely yugoslavia or serbia where it is a question of doing everything we can to foster democratisation and develop a longterm strategy for the europeanisation of serbia\n",
      "Ground-truth Frehch:  nous avons des pays comme la bosnieherzegovine et lalbanie dans lesquels letat fonctionne a peine et puis les deux defis decisifs a savoir la yougoslavie et la serbie ou il sagit dinstaurer fermement la democratisation et de developper une strategie a long terme sur leuropeanisation de la serbie\n",
      "Translation from seq2seq model:  je peut est le sout au sein de la c\n",
      "-\n",
      "English:        secondly kosovo where we will never achieve success unless elected political structures come into being there along with a longterm vision of what is to happen to this region which will never again be a serbian province\n",
      "Ground-truth Frehch:  deuxiemement le kosovo ou nous ne remporterons jamais aucun succes si aucune structure politique elue nest etablie labas et si aucune perspective nexiste sur ce quil adviendra a long terme de ce territoire qui ne sera plus jamais une province serbe\n",
      "Translation from seq2seq model:  je debat est clos\n",
      "\n",
      "-\n",
      "English:        we cannot afford to hang about over this issue any longer otherwise all we will be doing is tinkering about at the edges as it were\n",
      "Ground-truth Frehch:  nous ne devons plus tourner autour du pot sur cette question faute de quoi nous ne ferons que traiter les phenomenes superficiels\n",
      "Translation from seq2seq model:  je debat est clos\n",
      "\n",
      "-\n",
      "English:        but that is not the only factor adversely affecting the stability pact it is also suffering from a lack of credibility on the part of the international organisations and also the european union which owing to a confusion of competences wastage and mismanagement are displaying traits that would be dismissed as being typical of the balkans were it not for the fact that this is happening in our own organisations\n",
      "Ground-truth Frehch:  cependant le pacte de stabilite ne souffre pas seulement de cela mais egalement dun manque de credibilite dun manque de credibilite des organisations internationales ainsi que de lunion europeenne qui par la confusion des competences le gaspillage la mauvaise gestion vehiculent des notions que lon qualifierait de typiquement balkaniques si elles navaient pas lieu chez nous\n",
      "Translation from seq2seq model:  je pebat est clos\n",
      "\n",
      "-\n",
      "English:        we are losing a great deal in the way of credit and respect in this region and this respect and credibility are fundamentally even more important than the promises of funding that are made and then not kept\n",
      "Ground-truth Frehch:  nous perdons dans cette region enormement de credit de respect et ce respect et la credibilite sont au fond plus importants encore que les moyens financiers que lon promet et qui narrivent pas\n",
      "Translation from seq2seq model:  je debat est clos\n",
      "\n",
      "-\n",
      "English:        i truly believe we must put this situation in order in the european union primarily by strengthening the commission s role in the matter\n",
      "Ground-truth Frehch:  je suis vraiment davis que nous devons mettre de lordre dans lunion europeenne et cela passe avant tout par un renforcement de la commission a cet egard\n",
      "Translation from seq2seq model:  je debat est clos\n",
      "\n",
      "-\n",
      "English:        commissioner i have a great deal of faith in your work but i must say that we as a parliament must also ensure that you are able to give this work more indepth attention than you have been able to hitherto\n",
      "Ground-truth Frehch:  monsieur le commissaire jai une grande confiance dans votre travail mais je dois dire que nous en tant que parlement avons le devoir de veiller aussi a ce que vous puissiez faire avancer ce travail de maniere plus intensive que jusqua present\n",
      "Translation from seq2seq model:  je debat est clos\n",
      "\n",
      "-\n",
      "English:        what we need here is cooperation between our two institutions so as to strengthen the eu and its credibility\n",
      "Ground-truth Frehch:  a cet egard nous avons besoin dune cooperation entre nos deux institutions afin de renforcer lue et sa credibilite\n",
      "Translation from seq2seq model:  je debat est clos\n",
      "\n",
      "-\n",
      "English:        mr president i too consider this to be a satisfactory significant report\n",
      "Ground-truth Frehch:  monsieur le president je pense moi aussi qu il s agit d un rapport essentiel et correct\n",
      "Translation from seq2seq model:  je peut est clos\n",
      "\n",
      "-\n",
      "English:        i would however stress certain points\n",
      "Ground-truth Frehch:  je voudrais toutefois revenir brievement sur certains points\n",
      "Translation from seq2seq model:  je peut est clos\n",
      "\n",
      "-\n",
      "English:        we know that the countries of the southeast are a priority but we must be careful not to give ourselves more priorities than we can handle and not to establish conflicting priorities or we will lose credibility\n",
      "Ground-truth Frehch:  nous nous rappelons que les pays du sudest constituent une de nos priorites il faut cependant prendre garde a ne pas definir tellement de priorites que lon ne peut plus y repondre et a ne pas opposer deux priorites car nous y perdrions notre credibilite\n",
      "Translation from seq2seq model:  je debat est clos\n",
      "\n",
      "-\n",
      "English:        i refer to the mediterranean policy and the middle east peace process which are also priorities\n",
      "Ground-truth Frehch:  je pense ici a la politique mediterraneenne et au processus de paix au moyenorient qui constituent egalement des priorites\n",
      "Translation from seq2seq model:  je debat est clos\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print('-')\n",
    "    print('English:       ', test_en[i])\n",
    "    print('Ground-truth Frehch: ', test_fr[i])\n",
    "    print('Translation from seq2seq model: ', decoded_sentences[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('translation_without_attention.txt', 'w') as f:\n",
    "    for item in decoded_sentences:\n",
    "        f.write(\"%s\\n\" % item)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
