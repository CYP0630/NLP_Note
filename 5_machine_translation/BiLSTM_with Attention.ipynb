{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS584 Homework 5: Machine Translation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# With Attention Part"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* English to French"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data preparationÂ¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from unicodedata import normalize\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "load_doc: loading file \n",
    "to_line: cutting the the whole file to line by line\n",
    "clean_data: remove the symble, url, or other irrevalent information \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, mode='rt', encoding='utf-8')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text\n",
    "\n",
    "def to_line(doc):\n",
    "    lines = doc.strip().split('\\n')\n",
    "    pairs = [line.split('\\t') for line in  lines]\n",
    "    return lines\n",
    "\n",
    "def clean_data(lines):\n",
    "    cleaned = list()\n",
    "    # prepare regex for char filtering\n",
    "    re_print = re.compile('[^%s]' % re.escape(string.printable))\n",
    "    # prepare translation table for removing punctuation\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    for line in lines:\n",
    "        # normalize unicode characters\n",
    "        line = normalize('NFD', line).encode('utf-8', 'ignore')\n",
    "        #print(line)\n",
    "        line = line.decode('UTF-8')\n",
    "            \n",
    "        # tokenize on white space\n",
    "        line = line.split()\n",
    "            \n",
    "        # convert to lowercase\n",
    "        line = [word.lower() for word in line]\n",
    "            \n",
    "        # remove punctuation from each token\n",
    "        line = [word.translate(table) for word in line]\n",
    "            \n",
    "         # remove non-printable chars form each token\n",
    "        line = [re_print.sub('', w) for w in line]\n",
    "        #print(line)\n",
    "        # remove tokens with numbers in them\n",
    "        line = [word for word in line if word.isalpha()]\n",
    "            \n",
    "        # store as string\n",
    "        cleaned.append(' '.join(line))\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "English sentencens match with French sentences \n",
      "\n",
      "English sentence: 2007723 \n",
      "\n",
      "French sentence: 2007723 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Using the first 30000 sentences to contruct dataset\n",
    "data_len = 30000\n",
    "\n",
    "en = load_doc('europarl-v7.fr-en.en')\n",
    "en_lines = to_line(en)\n",
    "fr = load_doc('europarl-v7.fr-en.fr')\n",
    "fr_lines = to_line(fr)\n",
    "\n",
    "if len(en_lines) == len(fr_lines):\n",
    "    print('English sentencens match with French sentences \\n')\n",
    "    print('English sentence:', len(en_lines), '\\n')\n",
    "    print('French sentence:', len(fr_lines), '\\n')\n",
    "else :\n",
    "    print('English sentencens does not match with French sentences')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cleaning english part and french part \n",
    "clean_en = clean_data(en_lines)[0:data_len]\n",
    "clean_fr = clean_data(fr_lines)[0:data_len]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_en = np.array(clean_en)\n",
    "clean_fr = np.array(clean_fr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i should also like to make a few comments firstly mr berend regarding the assessment you have made of this sixth periodic report\n",
      "je voudrais a mon tour faire quelques observations dabord sur le jugement que vous portez monsieur le rapporteur sur ce sixieme rapport periodique\n"
     ]
    }
   ],
   "source": [
    "# A showcase \n",
    "print(clean_en[700])\n",
    "print(clean_fr[700])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of input_texts:  (30000,)\n",
      "Length of target_texts: (30000,)\n",
      "max length of input  sentences: 32\n",
      "max length of target sentences: 34\n"
     ]
    }
   ],
   "source": [
    "input_texts = clean_en\n",
    "target_texts = ['\\t' + text + '\\n' for text in clean_fr]\n",
    "\n",
    "print('Length of input_texts:  ' + str(input_texts.shape))\n",
    "print('Length of target_texts: ' + str(input_texts.shape))\n",
    "\n",
    "#This dataset includes lots of long sentence\n",
    "#But, if we set max length with a large number, CPU cannot handle that, so we set max_encode is 32\n",
    "#Becuase we add '\\t' and '\\n' to target, so the max_decode is 34\n",
    "max_encoder_seq_length = 32 \n",
    "max_decoder_seq_length = 34\n",
    "\n",
    "print('max length of input  sentences: %d' % (max_encoder_seq_length))\n",
    "print('max length of target sentences: %d' % (max_decoder_seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Text Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Text to Sequence\n",
    "* One-hot embedding the sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape of encoder_input_seq: (30000, 32)\n",
      "shape of input_token_index: 27\n",
      "shape of decoder_input_seq: (30000, 34)\n",
      "shape of target_token_index: 29\n"
     ]
    }
   ],
   "source": [
    "def text2sequences(max_len, lines):\n",
    "    tokenizer = Tokenizer(char_level=True, filters='')\n",
    "    tokenizer.fit_on_texts(lines)\n",
    "    seqs = tokenizer.texts_to_sequences(lines)\n",
    "    seqs_pad = pad_sequences(seqs, maxlen=max_len, padding='post')\n",
    "    return seqs_pad, tokenizer.word_index\n",
    "\n",
    "encoder_input_seq, input_token_index = text2sequences(max_encoder_seq_length, \n",
    "                                                      input_texts)\n",
    "decoder_input_seq, target_token_index = text2sequences(max_decoder_seq_length, \n",
    "                                                       target_texts)\n",
    "\n",
    "print('shape of encoder_input_seq: ' + str(encoder_input_seq.shape))\n",
    "print('shape of input_token_index: ' + str(len(input_token_index)))\n",
    "print('shape of decoder_input_seq: ' + str(decoder_input_seq.shape))\n",
    "print('shape of target_token_index: ' + str(len(target_token_index)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "num_encoder_tokens: 28\n",
      "num_decoder_tokens: 30\n"
     ]
    }
   ],
   "source": [
    "num_encoder_tokens = len(input_token_index) +1\n",
    "num_decoder_tokens = len(target_token_index) +1\n",
    "\n",
    "print('num_encoder_tokens: ' + str(num_encoder_tokens))\n",
    "print('num_decoder_tokens: ' + str(num_decoder_tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def onehot_encode(sequences, max_len, vocab_size):\n",
    "    n = len(sequences)\n",
    "    data = np.zeros((n, max_len, vocab_size))\n",
    "    for i in range(n):\n",
    "        data[i, :, :] = to_categorical(sequences[i], num_classes=vocab_size)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(30000, 32, 28)\n",
      "(30000, 34, 30)\n",
      "(30000, 34, 30)\n"
     ]
    }
   ],
   "source": [
    "encoder_input_data = onehot_encode(encoder_input_seq, max_encoder_seq_length, num_encoder_tokens)\n",
    "decoder_input_data = onehot_encode(decoder_input_seq, max_decoder_seq_length, num_decoder_tokens)\n",
    "\n",
    "decoder_target_seq = np.zeros(decoder_input_seq.shape)\n",
    "decoder_target_seq[:, 0:-1] = decoder_input_seq[:, 1:]\n",
    "decoder_target_data = onehot_encode(decoder_target_seq, \n",
    "                                    max_decoder_seq_length, \n",
    "                                    num_decoder_tokens)\n",
    "\n",
    "print(encoder_input_data.shape)\n",
    "print(decoder_input_data.shape)\n",
    "print(decoder_target_data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* **Split the dataset to training part and validation part**\n",
    "* **24000 training sentence, 6000 validation sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_indices = np.random.permutation(data_len)\n",
    "\n",
    "train_count = int(np.floor(data_len * 0.8))\n",
    "train_indices = rand_indices[0:train_count]\n",
    "val_indices = rand_indices[train_count:data_len]\n",
    "\n",
    "input_texts_train = input_texts[train_indices]\n",
    "target_texts_train = np.asarray(target_texts)[train_indices]\n",
    "\n",
    "encoder_input_train = encoder_input_data[train_indices]\n",
    "decoder_input_train = decoder_input_data[train_indices]\n",
    "decoder_target_train = decoder_target_data[train_indices]\n",
    "\n",
    "input_texts_val = input_texts[val_indices]\n",
    "target_texts_val = np.asarray(target_texts)[val_indices]\n",
    "encoder_input_val = encoder_input_data[val_indices]\n",
    "decoder_input_val = decoder_input_data[val_indices]\n",
    "decoder_target_val = decoder_target_data[val_indices]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM\n",
    "from keras.models import Model\n",
    "from keras.layers import Bidirectional, Concatenate\n",
    "import tensorflow as tf\n",
    "\n",
    "latent_dim = 256\n",
    "\n",
    "# inputs of the encoder network\n",
    "encoder_inputs = Input(shape=(None, num_encoder_tokens), \n",
    "                       name='encoder_inputs')\n",
    "\n",
    "# set the LSTM layer\n",
    "#encoder_lstm = LSTM(latent_dim, return_state=True,dropout=0.5, name='encoder_lstm')\n",
    "#encoder_outputs, encoder_state_h, encoder_state_c = encoder_lstm(encoder_inputs)\n",
    "\n",
    "\n",
    "\n",
    "encoder_bilstm = Bidirectional(LSTM(latent_dim, return_state=True,dropout=0.5, name='encoder_lstm'))\n",
    "encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_bilstm(encoder_inputs)\n",
    "\n",
    "encoder_state_h = Concatenate()([forward_h, backward_h])\n",
    "encoder_state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "forward_states = [forward_h, forward_c]\n",
    "encoder_state = [encoder_state_h,encoder_state_c]\n",
    "\n",
    "# build the encoder network model\n",
    "encoder_model = Model(inputs=encoder_inputs, \n",
    "                      outputs=encoder_state,\n",
    "                      name='encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"encoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, None, 28)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) [(None, 512), (None, 583680      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 512)          0           bidirectional_1[0][1]            \n",
      "                                                                 bidirectional_1[0][3]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 512)          0           bidirectional_1[0][2]            \n",
      "                                                                 bidirectional_1[0][4]            \n",
      "==================================================================================================\n",
      "Total params: 583,680\n",
      "Trainable params: 583,680\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(encoder_model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=encoder_model, show_shapes=False,\n",
    "    to_file='encoder.pdf'\n",
    ")\n",
    "\n",
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 3.2 Decoder network+ Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, LSTM, Dense, Multiply, Activation\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs of the decoder network\n",
    "decoder_input_h = Input(shape=(latent_dim*2,), name='decoder_input_h')\n",
    "decoder_input_c = Input(shape=(latent_dim*2,), name='decoder_input_c')\n",
    "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
    "\n",
    "\n",
    "\n",
    "# set the LSTM layer\n",
    "decoder_lstm = LSTM(latent_dim*2, return_sequences=True, \n",
    "                    return_state=True, dropout=0.5, name='decoder_lstm')\n",
    "\n",
    "\n",
    "# set attention layer\n",
    "wc = Dense(latent_dim*2, activation='tanh')\n",
    "# set the dense layer\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax', name='decoder_dense')\n",
    "\n",
    "decoder_lstm_outputs, decoder_state_h, decoder_state_c = decoder_lstm(decoder_input_x, \n",
    "                                                                      initial_state=[decoder_input_h, decoder_input_c])\n",
    "\n",
    "\n",
    "Wa = Dense(latent_dim*2)\n",
    "score = Multiply()([decoder_lstm_outputs, Wa(decoder_input_h)])\n",
    "alignment = Activation('softmax')(score)\n",
    "context = Multiply()([alignment, decoder_input_h])\n",
    "\n",
    "lstm_out = Concatenate()([context, decoder_lstm_outputs])\n",
    "\n",
    "\n",
    "lstm_out = wc(lstm_out)\n",
    "\n",
    "\n",
    "ws = Dense(num_decoder_tokens)\n",
    "#logits = ws(lstm_out)\n",
    "\n",
    "decoder_outputs1 = decoder_dense(lstm_out)\n",
    "\n",
    "\n",
    "\n",
    "# build the decoder network model\n",
    "decoder_model = Model(inputs=[decoder_input_x, decoder_input_h, decoder_input_c],\n",
    "                      outputs=[decoder_outputs1, decoder_state_h, decoder_state_c],\n",
    "                      name='decoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"decoder\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "decoder_input_x (InputLayer)    (None, None, 30)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_h (InputLayer)    (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_c (InputLayer)    (None, 512)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 512),  1112064     decoder_input_x[0][0]            \n",
      "                                                                 decoder_input_h[0][0]            \n",
      "                                                                 decoder_input_c[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 512)          262656      decoder_input_h[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, None, 512)    0           decoder_lstm[0][0]               \n",
      "                                                                 dense_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, None, 512)    0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply_2 (Multiply)           (None, None, 512)    0           activation_1[0][0]               \n",
      "                                                                 decoder_input_h[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_3 (Concatenate)     (None, None, 1024)   0           multiply_2[0][0]                 \n",
      "                                                                 decoder_lstm[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, None, 512)    524800      concatenate_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 30)     15390       dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,914,910\n",
      "Trainable params: 1,914,910\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot, plot_model\n",
    "\n",
    "SVG(model_to_dot(decoder_model, show_shapes=False).create(prog='dot', format='svg'))\n",
    "\n",
    "plot_model(\n",
    "    model=decoder_model, show_shapes=False,\n",
    "    to_file='decoder.pdf'\n",
    ")\n",
    "\n",
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# input layers\n",
    "encoder_input_x = Input(shape=(None, num_encoder_tokens), name='encoder_input_x')\n",
    "decoder_input_x = Input(shape=(None, num_decoder_tokens), name='decoder_input_x')\n",
    "\n",
    "# connect encoder to decoder\n",
    "encoder_final_states = encoder_model([encoder_input_x])\n",
    "\n",
    "decoder_pred,_,_ = decoder_model([decoder_input_x]+encoder_final_states)\n",
    "\n",
    "\n",
    "\n",
    "#decoder_lstm_output, _, _ = decoder_lstm(decoder_input_x, initial_state=encoder_final_states)\n",
    "#decoder_pred = decoder_dense(decoder_lstm_output)\n",
    "\n",
    "\n",
    "\n",
    "model = Model(inputs=[encoder_input_x, decoder_input_x], \n",
    "              outputs=decoder_pred, \n",
    "              name='model_training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_training\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_input_x (InputLayer)    (None, None, 28)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_input_x (InputLayer)    (None, None, 30)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder (Model)                 [(None, 512), (None, 583680      encoder_input_x[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "decoder (Model)                 [(None, None, 30), ( 1914910     decoder_input_x[0][0]            \n",
      "                                                                 encoder[1][0]                    \n",
      "                                                                 encoder[1][1]                    \n",
      "==================================================================================================\n",
      "Total params: 2,498,590\n",
      "Trainable params: 2,498,590\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training and Evaluate Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 24000 samples, validate on 6000 samples\n",
      "Epoch 1/50\n",
      "24000/24000 [==============================] - 32s 1ms/step - loss: 2.4920 - val_loss: 2.0277\n",
      "Epoch 2/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 2.1876 - val_loss: 1.8262\n",
      "Epoch 3/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 2.0717 - val_loss: 1.7125\n",
      "Epoch 4/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.9988 - val_loss: 1.6582\n",
      "Epoch 5/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.9451 - val_loss: 1.5451\n",
      "Epoch 6/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.9045 - val_loss: 1.5055\n",
      "Epoch 7/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.8691 - val_loss: 1.4883\n",
      "Epoch 8/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.8367 - val_loss: 1.4275\n",
      "Epoch 9/50\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 1.8135 - val_loss: 1.4036\n",
      "Epoch 10/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.7989 - val_loss: 1.3688\n",
      "Epoch 11/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.7732 - val_loss: 1.3459\n",
      "Epoch 12/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.7548 - val_loss: 1.3247\n",
      "Epoch 13/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.7445 - val_loss: 1.3112\n",
      "Epoch 14/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.7298 - val_loss: 1.2968\n",
      "Epoch 15/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.7148 - val_loss: 1.2808\n",
      "Epoch 16/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.7033 - val_loss: 1.2690\n",
      "Epoch 17/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.6946 - val_loss: 1.2571\n",
      "Epoch 18/50\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 1.6829 - val_loss: 1.2464\n",
      "Epoch 19/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.6781 - val_loss: 1.2408\n",
      "Epoch 20/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.6699 - val_loss: 1.2296\n",
      "Epoch 21/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.6568 - val_loss: 1.2188\n",
      "Epoch 22/50\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 1.6510 - val_loss: 1.2165\n",
      "Epoch 23/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.6402 - val_loss: 1.2062\n",
      "Epoch 24/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.6339 - val_loss: 1.1967\n",
      "Epoch 25/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.6285 - val_loss: 1.1953\n",
      "Epoch 26/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.6210 - val_loss: 1.1910\n",
      "Epoch 27/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.6190 - val_loss: 1.1783\n",
      "Epoch 28/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.6080 - val_loss: 1.1745\n",
      "Epoch 29/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.6037 - val_loss: 1.1747\n",
      "Epoch 30/50\n",
      "24000/24000 [==============================] - 29s 1ms/step - loss: 1.6012 - val_loss: 1.1681\n",
      "Epoch 31/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.5933 - val_loss: 1.1647\n",
      "Epoch 32/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.5867 - val_loss: 1.1605\n",
      "Epoch 33/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.5883 - val_loss: 1.1641\n",
      "Epoch 34/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.5816 - val_loss: 1.1483\n",
      "Epoch 35/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.5753 - val_loss: 1.1475\n",
      "Epoch 36/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.5712 - val_loss: 1.1452\n",
      "Epoch 37/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.5634 - val_loss: 1.1431\n",
      "Epoch 38/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.5679 - val_loss: 1.1389\n",
      "Epoch 39/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.5567 - val_loss: 1.1397\n",
      "Epoch 40/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.5549 - val_loss: 1.1270\n",
      "Epoch 41/50\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 1.5519 - val_loss: 1.1324\n",
      "Epoch 42/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.5477 - val_loss: 1.1245\n",
      "Epoch 43/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.5399 - val_loss: 1.1244\n",
      "Epoch 44/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.5381 - val_loss: 1.1203\n",
      "Epoch 45/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.5370 - val_loss: 1.1235\n",
      "Epoch 46/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.5324 - val_loss: 1.1189\n",
      "Epoch 47/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.5277 - val_loss: 1.1165\n",
      "Epoch 48/50\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 1.5275 - val_loss: 1.1387\n",
      "Epoch 49/50\n",
      "24000/24000 [==============================] - 30s 1ms/step - loss: 1.5204 - val_loss: 1.1121\n",
      "Epoch 50/50\n",
      "24000/24000 [==============================] - 31s 1ms/step - loss: 1.5164 - val_loss: 1.1086\n"
     ]
    }
   ],
   "source": [
    "history = model.fit([encoder_input_train, decoder_input_train],  # training data\n",
    "                      decoder_target_train,                       # labels (left shift of the target sequences)\n",
    "                      batch_size=64, epochs=50, \n",
    "                      validation_data= ([encoder_input_val, decoder_input_val],decoder_target_val))\n",
    "\n",
    "model.save('seq2seq.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(len(loss))\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training Loss')\n",
    "plt.plot(epochs, val_loss, 'r', label='Validation Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Calculate BLEU Score on Val Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    target_seq[0, 0, target_token_index['\\t']] = 1.\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # this line of code is greedy selection\n",
    "        # try to use multinomial sampling instead (with temperature)\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        if sampled_token_index == 0:\n",
    "            output_tokens[0, -1, sampled_token_index] = 0\n",
    "            sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        if (sampled_char == '\\n' or\n",
    "           len(decoded_sentence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_sentences = []\n",
    "for i in range(encoder_input_val.shape[0]):\n",
    "    decoded_sentences.append(decode_sequence(encoder_input_val[i: i + 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.033205802915126745\n"
     ]
    }
   ],
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "import nltk\n",
    "\n",
    "val_scores = []\n",
    "for i in range(target_texts_val.shape[0]):\n",
    "    references = []\n",
    "    split_result = target_texts_val[i][1:-1].split()\n",
    "    references.append(split_result)\n",
    "    candidate = decoded_sentences[i].split()\n",
    "    val_scores.append(sentence_bleu(references, candidate, smoothing_function=SmoothingFunction().method2))\n",
    "\n",
    "val_bleu_score = np.mean(val_scores)\n",
    "print(val_bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* We take 1000 sentences from 40000 to 41000 in original file as the test part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_en = clean_data(en_lines)[data_len+10000:data_len+11000]\n",
    "test_fr = clean_data(fr_lines)[data_len+10000:data_len+11000]\n",
    "\n",
    "test_en = np.array(test_en)\n",
    "test_fr = np.array(test_fr)\n",
    "target_texts_test = ['\\t' + text + '\\n' for text in test_fr]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translation_sentence(input_sentence):\n",
    "    input_seq_test, test_input_token = text2sequences(len(input_sentence), input_sentence)\n",
    "    max_encoder_seq_length = max(len(line) for line in input_seq_test)\n",
    "    input_x = onehot_encode(input_seq_test, max_encoder_seq_length, num_encoder_tokens)\n",
    "    translated_sentence = decode_sequence(input_x)\n",
    "\n",
    "    return translated_sentence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_test = []\n",
    "for i in range(len(target_texts_test)):\n",
    "    candidate = translation_sentence(test_en[i])\n",
    "    reference = target_texts_test[i]\n",
    "    score = sentence_bleu(reference, candidate, \n",
    "                          smoothing_function=SmoothingFunction().method2)\n",
    "    scores_test.append(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5940973913275965\n"
     ]
    }
   ],
   "source": [
    "test_bleu_score = np.mean(scores_test)\n",
    "print(test_bleu_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Print 20 Sentences Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "lstm_result = []\n",
    "for line in open(\"translation_without_attention.txt\",\"r\"):\n",
    "    lstm_result.append(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "English:        i think that the donor conference makes this possible or seems to at least\n",
      "Ground-truth Frehch:  je pense que la conference des donateurs offre ou semble offrir cette possibilite\n",
      "Translation from seq2seq model:  je peut est clos\n",
      "\n",
      "Translation from seq2seq plus attention:  le projet de paix du moyenorient\n",
      "\n",
      "-\n",
      "English:        the third point i would like to make here is that there are parts of greece yes unfortunately greece for example the prefecture of pieria a tourist province in central macedonia which has suffered enormous economic damage because tourists no longer come to that area\n",
      "Ground-truth Frehch:  troisieme point il existe des parties de grece oui de grece malheureusement comme le nome de pierie un nome touristique de la macedoine centrale qui ont subi un enorme prejudice economique du fait que les visiteurs ont deserte la region\n",
      "Translation from seq2seq model:  \n",
      "\n",
      "Translation from seq2seq plus attention:  res des cinorites et de la pache\n",
      "\n",
      "-\n",
      "English:        such issues should be considered by the european union as well\n",
      "Ground-truth Frehch:  c est un probleme qu il convient aussi d examiner dans le cadre de l union europeenne\n",
      "Translation from seq2seq model:  je debat est clos\n",
      "\n",
      "Translation from seq2seq plus attention:  nous voulons de contre de tamps\n",
      "\n",
      "-\n",
      "English:        mr president i am not about to start arguing the toss with mr swoboda over where the balkans begin\n",
      "Ground-truth Frehch:  monsieur le president je ne souhaite pas me disputer avec mon collegue swoboda sur la question de savoir ou commencent les balkans\n",
      "Translation from seq2seq model:  \n",
      "\n",
      "Translation from seq2seq plus attention:  nous voulons de contre de tamps\n",
      "\n",
      "-\n",
      "English:        in fact it was once said that the balkans begin at the rennweg in vienna\n",
      "Ground-truth Frehch:  on a meme dit quils commencaient aux abords de vienne\n",
      "Translation from seq2seq model:  je peut est clos\n",
      "\n",
      "Translation from seq2seq plus attention:  la seance de la president gagan\n",
      "\n",
      "-\n",
      "English:        but leaving that aside the fact remains that this stability pact is of course adversely affected by the heterogeneity of its states\n",
      "Ground-truth Frehch:  mais si lon excepte cette question le fait est que ce pacte de stabilite souffre naturellement de lheterogeneite des etats concernes\n",
      "Translation from seq2seq model:  \n",
      "\n",
      "Translation from seq2seq plus attention:  je vous avons une moint en place\n",
      "\n",
      "-\n",
      "English:        we have first and second wave candidate countries\n",
      "Ground-truth Frehch:  nous avons des pays candidats a ladhesion faisant partie du premier et du deuxieme groupe\n",
      "Translation from seq2seq model:  je debat est clos\n",
      "\n",
      "Translation from seq2seq plus attention:  cest une question important\n",
      "\n",
      "-\n",
      "English:        we have countries such as croatia and macedonia that are about to overtake candidate countries\n",
      "Ground-truth Frehch:  nous avons des pays comme la croatie et la macedoine qui sont sur le point de depasser les candidats a ladhesion\n",
      "Translation from seq2seq model:  \n",
      "\n",
      "Translation from seq2seq plus attention:  en en en en en en en a problemes\n",
      "\n",
      "-\n",
      "English:        we have states such as bosniaherzegovina and albania in which the state is scarcely functioning and then the two decisive challenges namely yugoslavia or serbia where it is a question of doing everything we can to foster democratisation and develop a longterm strategy for the europeanisation of serbia\n",
      "Ground-truth Frehch:  nous avons des pays comme la bosnieherzegovine et lalbanie dans lesquels letat fonctionne a peine et puis les deux defis decisifs a savoir la yougoslavie et la serbie ou il sagit dinstaurer fermement la democratisation et de developper une strategie a long terme sur leuropeanisation de la serbie\n",
      "Translation from seq2seq model:  je peut est clos\n",
      "\n",
      "Translation from seq2seq plus attention:   europe de seutenir les choses\n",
      "\n",
      "-\n",
      "English:        secondly kosovo where we will never achieve success unless elected political structures come into being there along with a longterm vision of what is to happen to this region which will never again be a serbian province\n",
      "Ground-truth Frehch:  deuxiemement le kosovo ou nous ne remporterons jamais aucun succes si aucune structure politique elue nest etablie labas et si aucune perspective nexiste sur ce quil adviendra a long terme de ce territoire qui ne sera plus jamais une province serbe\n",
      "Translation from seq2seq model:  \n",
      "\n",
      "Translation from seq2seq plus attention:  la protection de l environnement\n",
      "\n",
      "-\n",
      "English:        we cannot afford to hang about over this issue any longer otherwise all we will be doing is tinkering about at the edges as it were\n",
      "Ground-truth Frehch:  nous ne devons plus tourner autour du pot sur cette question faute de quoi nous ne ferons que traiter les phenomenes superficiels\n",
      "Translation from seq2seq model:  je debat est clos\n",
      "\n",
      "Translation from seq2seq plus attention:  nous voulons pour un autre evite\n",
      "\n",
      "-\n",
      "English:        but that is not the only factor adversely affecting the stability pact it is also suffering from a lack of credibility on the part of the international organisations and also the european union which owing to a confusion of competences wastage and mismanagement are displaying traits that would be dismissed as being typical of the balkans were it not for the fact that this is happening in our own organisations\n",
      "Ground-truth Frehch:  cependant le pacte de stabilite ne souffre pas seulement de cela mais egalement dun manque de credibilite dun manque de credibilite des organisations internationales ainsi que de lunion europeenne qui par la confusion des competences le gaspillage la mauvaise gestion vehiculent des notions que lon qualifierait de typiquement balkaniques si elles navaient pas lieu chez nous\n",
      "Translation from seq2seq model:  \n",
      "\n",
      "Translation from seq2seq plus attention:  le parlement a orte du probleme\n",
      "\n",
      "-\n",
      "English:        we are losing a great deal in the way of credit and respect in this region and this respect and credibility are fundamentally even more important than the promises of funding that are made and then not kept\n",
      "Ground-truth Frehch:  nous perdons dans cette region enormement de credit de respect et ce respect et la credibilite sont au fond plus importants encore que les moyens financiers que lon promet et qui narrivent pas\n",
      "Translation from seq2seq model:  je peut est clos\n",
      "\n",
      "Translation from seq2seq plus attention:  ne souhait pas encore plus lirge\n",
      "\n",
      "-\n",
      "English:        i truly believe we must put this situation in order in the european union primarily by strengthening the commission s role in the matter\n",
      "Ground-truth Frehch:  je suis vraiment davis que nous devons mettre de lordre dans lunion europeenne et cela passe avant tout par un renforcement de la commission a cet egard\n",
      "Translation from seq2seq model:  \n",
      "\n",
      "Translation from seq2seq plus attention:  reprise des questions\n",
      "\n",
      "-\n",
      "English:        commissioner i have a great deal of faith in your work but i must say that we as a parliament must also ensure that you are able to give this work more indepth attention than you have been able to hitherto\n",
      "Ground-truth Frehch:  monsieur le commissaire jai une grande confiance dans votre travail mais je dois dire que nous en tant que parlement avons le devoir de veiller aussi a ce que vous puissiez faire avancer ce travail de maniere plus intensive que jusqua present\n",
      "Translation from seq2seq model:  je peut est clos\n",
      "\n",
      "Translation from seq2seq plus attention:   au sein de la protection social\n",
      "\n",
      "-\n",
      "English:        what we need here is cooperation between our two institutions so as to strengthen the eu and its credibility\n",
      "Ground-truth Frehch:  a cet egard nous avons besoin dune cooperation entre nos deux institutions afin de renforcer lue et sa credibilite\n",
      "Translation from seq2seq model:  \n",
      "\n",
      "Translation from seq2seq plus attention:  in de la commission de la pharte\n",
      "\n",
      "-\n",
      "English:        mr president i too consider this to be a satisfactory significant report\n",
      "Ground-truth Frehch:  monsieur le president je pense moi aussi qu il s agit d un rapport essentiel et correct\n",
      "Translation from seq2seq model:  je peut est le sout au sein de la c\n",
      "\n",
      "Translation from seq2seq plus attention:  le parlement a opte la resolution\n",
      "\n",
      "-\n",
      "English:        i would however stress certain points\n",
      "Ground-truth Frehch:  je voudrais toutefois revenir brievement sur certains points\n",
      "Translation from seq2seq model:  je debat est clos\n",
      "\n",
      "Translation from seq2seq plus attention:  in est pas une position commune\n",
      "\n",
      "-\n",
      "English:        we know that the countries of the southeast are a priority but we must be careful not to give ourselves more priorities than we can handle and not to establish conflicting priorities or we will lose credibility\n",
      "Ground-truth Frehch:  nous nous rappelons que les pays du sudest constituent une de nos priorites il faut cependant prendre garde a ne pas definir tellement de priorites que lon ne peut plus y repondre et a ne pas opposer deux priorites car nous y perdrions notre credibilite\n",
      "Translation from seq2seq model:  \n",
      "\n",
      "Translation from seq2seq plus attention:  a parlement a opte la resolution\n",
      "\n",
      "-\n",
      "English:        i refer to the mediterranean policy and the middle east peace process which are also priorities\n",
      "Ground-truth Frehch:  je pense ici a la politique mediterraneenne et au processus de paix au moyenorient qui constituent egalement des priorites\n",
      "Translation from seq2seq model:  je debat est clos\n",
      "\n",
      "Translation from seq2seq plus attention:  le parlement adopte la resolutie\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(20):\n",
    "    print('-')\n",
    "    print('English:       ', test_en[i])\n",
    "    print('Ground-truth Frehch: ', test_fr[i])\n",
    "    print('Translation from seq2seq model: ', lstm_result[i])\n",
    "    print('Translation from seq2seq plus attention: ', decoded_sentences[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
